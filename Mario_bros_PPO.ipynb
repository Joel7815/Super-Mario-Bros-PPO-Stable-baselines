{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ea8140-c917-443e-8da4-90f44ac2279c",
   "metadata": {},
   "source": [
    "## Start by installing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf615ec6-7b3f-4cd6-8dea-933787bce2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b61511-5522-431a-82a8-e88b8cf992d8",
   "metadata": {},
   "source": [
    "## Don't forget to restart the kernel when the installation is completed, then you can execute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5988bb74-c5e1-4856-887e-6ee4027eabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "import os\n",
    "import gym\n",
    "import csv\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv, VecVideoRecorder, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from gym_super_mario_bros import make\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, TimeLimit, AtariPreprocessing\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afcb4c0-ab21-4975-9a00-98aec921b9ef",
   "metadata": {},
   "source": [
    "Tensorboard graphs and your models will be saved in LOG_DIR, rewards for each episodes \n",
    "will be saved in reward_dir. It's not necessary to save the rewards, but they can be\n",
    "a good indication of how well your agent is performing (especially if you're new to\n",
    "reinforcement learning and you don't posses a good mathematical background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82842545-e823-4c6c-abdd-93f84ee6c58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOG_DIR = './mariologs/'\n",
    "reward_dir = './mariologreward/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999ba7b-ab09-46de-80d7-8b711c97d5b6",
   "metadata": {},
   "source": [
    "## Preprocess of the environement\n",
    "\n",
    "I used RIGHT_ONLY wich gives my agent only two actions : go right OR go right and jump.\n",
    "If you want to change the control, just check the documentation : https://github.com/Kautenja/gym-super-mario-bros\n",
    "If you want to change the level in wich Mario is playing you can change the values of the gym.make function\n",
    "according to this tempalte : 'SuperMarioBros-<world>-<stage>-v<version>'.\n",
    "Timelimit is use so that the episode stops after 2000 steps, this prevents Mario getting stuck to long\n",
    "in front of a pipe.\n",
    "Make_vec_env is used to be able to have parralel environements, wich accelerates the training.\n",
    "VecFrameStack is used so that the agent receives the 4 last frames when he plays and have to take\n",
    "an action. This allows him to have a \"sense\" of direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04a0b14-f860-467a-bcae-352db2734fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make('SuperMarioBros-1-1-v0')\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = TimeLimit(env, max_episode_steps=2000)\n",
    "    return env\n",
    "# Create a vectorized environment using make_vec_env and the make_env function\n",
    "env = make_vec_env(make_env, n_envs=2, monitor_dir=reward_dir)\n",
    "# Wrap the environment with VecFrameStack\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb96d7-fb89-481a-8193-5f19489f11ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555abd1-f421-4b1d-87ec-a08dfc7dea65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the adjusted save frequency based on the number of environments (2)\n",
    "save_freq=400000\n",
    "adjusted_save_freq = max(save_freq // 2, 1)\n",
    "# This code creates a CheckpointCallback object with a frequency of \"save_freq\" steps\n",
    "# and saves the best model to the LOG_DIR directory\n",
    "LOG_DIR = './mariologsgen4.6/'\n",
    "checkpoint_callback = CheckpointCallback(save_freq=adjusted_save_freq, save_path=LOG_DIR)\n",
    "# Create the PPO agent\n",
    "# Defining the model,we had frames (4) as input so we use CnnPolicy\n",
    "model = PPO(\n",
    "    policy=\"CnnPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.00003,\n",
    "    tensorboard_log=LOG_DIR,\n",
    ")\n",
    "# Train the model\n",
    "model.learn(total_timesteps=20000000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8580529-779a-4c58-85f5-5e00470d9d2d",
   "metadata": {},
   "source": [
    "## Start the training of a previously trained model\n",
    "\n",
    "When you want to restart the training it's very important that your model is trained\n",
    "in exactly the same environement he started its training in. This is especially important \n",
    "when using online solutions like Google Colab or Papersapce Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71176fc9-64db-4766-a3c6-373d32fde117",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './mariologsgen4.5/'\n",
    "reward_dir = './mariologsgen4.5reward/'\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make('SuperMarioBros-1-1-v0')\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = TimeLimit(env, max_episode_steps=2000)\n",
    "    return env\n",
    "# Create a vectorized environment using make_vec_env and the make_env function, two parralel environements are created\n",
    "env = make_vec_env(make_env, n_envs=2, monitor_dir=reward_dir)\n",
    "# Wrap the environment with VecFrameStack\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# This code creates a CheckpointCallback object with a frequency of \"save_freq\" steps\n",
    "# and saves the best model to the LOG_DIR directory\n",
    "save_freq=200000\n",
    "adjusted_save_freq = max(save_freq // 2, 1)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=adjusted_save_freq, save_path=LOG_DIR)\n",
    "\n",
    "# Load the saved model\n",
    "model = PPO.load(\"path/to/your/saved/model\", env=env)\n",
    "\n",
    "# Print the number of timesteps the model has trained for before continuing the training, just to make sure\n",
    "# you don't start from 0\n",
    "print(\"Number of timesteps before continuing the training:\", model.num_timesteps)\n",
    "\n",
    "# Train the model for an additional \"total_timesteps\" \n",
    "model.learn(total_timesteps=17000000, reset_num_timesteps=False, callback=checkpoint_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d45341-9ff4-45a5-9d9d-2887bebd64d2",
   "metadata": {},
   "source": [
    "## That's the part where you can record videos of the agent in the environement\n",
    "\n",
    "Being able to visualize your agent in its environement in a notebook cell is a true nightmare, this is why I did\n",
    "this bit of code to be able to record a video instead. Don't forget to create exactly the same environment in wich\n",
    "your agent was trained in. The only modification you should do is change the number of environement to one instead of two or more if you used parralel environments.\n",
    "If you want to see how the model behave in another level, you can change the gym.make argument. For example, if\n",
    "you want to see how the agent behave in world 2, level 2 : 'SuperMarioBros-2-2-v0'. For more information\n",
    "just check : https://github.com/Kautenja/gym-super-mario-bros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d66cc9-6b39-4fb4-afe9-d548ba4bf669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make('SuperMarioBros-1-1-v0')\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = TimeLimit(env, max_episode_steps=2000)\n",
    "    return env\n",
    "# Create a vectorized environment using make_vec_env and the make_env function\n",
    "env = make_vec_env(make_env, n_envs=1)\n",
    "# Wrap the environment with VecFrameStack\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Load the saved model\n",
    "model = PPO.load(\"mariologsgen4.5/rl_model_1000000_steps.zip\")\n",
    "\n",
    "# Create a video recorder\n",
    "video_length = 160000\n",
    "video_fps = 30\n",
    "video_folder = \"videos/\"\n",
    "video_name = \"mario.mp4\"\n",
    "env = VecVideoRecorder(env, video_folder, record_video_trigger=lambda x: x == 0, video_length=video_length, name_prefix=video_name)\n",
    "\n",
    "# Run the model in the environment\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Save the video\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d74e5-e365-4e99-8514-e9e50fc738b3",
   "metadata": {},
   "source": [
    "## This is the part where you can evaluate the performance of your agent\n",
    "\n",
    "The evaluation method : getting the mean, the standard deviation and the \n",
    "number of flags reached (succesful completion of the level) are based on\n",
    "the master thesis from Zone wich you can access in my Github repo.\n",
    "\n",
    "As always you can change the world or level by changing the gym.make\n",
    "argument. Also remember to make the exact same preprocess of the environement\n",
    "you did when first training your model, just change the number of environment\n",
    "\"n_env\" to one if you used two or more parallel environment during the training.\n",
    "If you want to evaluate the model I created just change the path of the saved \n",
    "model to the one I uploaded on my Github repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eea5ca-8062-44f9-9c7e-2e3bfe203878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make('SuperMarioBros-1-1-v0')\n",
    "    env = JoypadSpace(env, RIGHT_ONLY)\n",
    "    env = TimeLimit(env, max_episode_steps=2000)\n",
    "    return env\n",
    "\n",
    "# Create a vectorized environment using make_vec_env and the make_env function\n",
    "env = make_vec_env(make_env, n_envs=1)\n",
    "\n",
    "# Wrap the environment with VecFrameStack\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Load the saved model\n",
    "model = PPO.load(\"path/to/your/saved/model\")\n",
    "\n",
    "# Run the model for 1000 episodes\n",
    "episode_rewards = []\n",
    "flags_reached = 0\n",
    "\n",
    "for i in range(1000):\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check if the flag has been reached\n",
    "        if info[0]['flag_get']:\n",
    "            flags_reached += 1\n",
    "            print(\"Agent completed the level in episode {}\".format(i+1))\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    print(\"Reward for episode {}: {}\".format(i+1, episode_reward))\n",
    "\n",
    "# Calculate statistics\n",
    "mean_reward = np.mean(episode_rewards)\n",
    "std_reward = np.std(episode_rewards)\n",
    "max_reward = np.max(episode_rewards)\n",
    "\n",
    "# Print statistics\n",
    "print(\"Mean reward over 1000 episodes:\", mean_reward)\n",
    "print(\"Standard deviation of rewards:\", std_reward)\n",
    "print(\"Maximum reward:\", max_reward)\n",
    "print(\"Number of flags reached:\", flags_reached)\n",
    "\n",
    "# Write statistics to a CSV file\n",
    "csv_file = \"statistics.csv\"\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Episode\", \"Reward\"])\n",
    "    for i, reward in enumerate(episode_rewards):\n",
    "        writer.writerow([i+1, reward])\n",
    "    writer.writerow([])  # Empty row\n",
    "    writer.writerow([\"Mean Reward\", mean_reward])\n",
    "    writer.writerow([\"Standard Deviation\", std_reward])\n",
    "    writer.writerow([\"Maximum Reward\", max_reward])\n",
    "    writer.writerow([\"Flags Reached\", flags_reached])\n",
    "\n",
    "print(\"Statistics written to\", csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbc5b9-5cde-4287-9ed7-daca2e9fee23",
   "metadata": {},
   "source": [
    "## Last note for visualizing tensorboard logs\n",
    "\n",
    "You should have tensorboard installed on your computer. You should know the path to\n",
    "the tensorboard logs. From there you can just execute \"python -m tensorboard.main --logdir=C:\\Users\\path\\to\\your\\tensorboardlogs\". The console will give you a link that you can put in your browser and you'll\n",
    "be able to visualize the graphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
